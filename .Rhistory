View(normalizedPoints)
remove(normalizedPoints)
View(recepcionistasDB)
normalizedDB <- apply(recepcionistasDB, [,2], 1 , scale)
normalizedDB <- apply(recepcionistasDB[,2], 1 , scale)
install.packages("scales")
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas)
library("scales")
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas)
View(pointsRecepcionistas)
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas, to c (-1,1))
normalizedRecepcionistasDB <- rescale(pointsRecepcionistas, to=c(-1,1))
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, sum)
normalizedRecepcionistasDB
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale)
normalizedRecepcionistasDB
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale, (-1,1))
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale(-1,1))
normalizedRecepcionistasDB <- apply(pointsRecepcionistas, 2, rescale, to=c(-1,1))
normalizedRecepcionistasDB
rowMeans <- apply(normalizedRecepcionistasDB, 2 , colMeans)
View(normalizedRecepcionistasDB)
pointsRecepcionistas
install.packages("ca")
library("ca")
fum = matrix  (c(4,2,3,2,4,3,7,4,25,10,12,4,18,24,33,13,10,6,7,2))
fum
View fum
View(fum)
fum = matrix  (c(4,2,3,2,4,3,7,4,25,10,12,4,18,24,33,13,10,6,7,2), nrow = 5, ncol = 4, byrow = TRUE)
View(fum)
tabum = addmargins(fum)
View(tabum)
colnames(fum) = c("NoFuma", "Poco", "Medio", "Mucho", "TotalFila")
colnames(tabum) = c("NoFuma", "Poco", "Medio", "Mucho", "TotalFila")
rownames(tabum) = c("G.Senior", "G.Junior", "EmpSenior", "EmpJunior", "Secretarira", "Total_Col")
View(tabum)
objeto = ca(tabum, nd=2)
plot(objeto, main="Biplot Simétrico")
Arbequina=c(34.5, 20.1, 21.8 ,18.2 ,19.5 ,20.2,22.5 ,23.9 ,22.1 ,24.2)
Carolea=c (16.4, 14.8, 17.8, 12.3, 11.9, 15.5, 13.4,16 ,15.8 ,16.2)
shapiro.test(Arbequina) # testeamos la normalidad de los datos
shapiro.test(CArolea) # testeamos la normalidad de los datos
shapiro.test(Carolea) # testeamos la normalidad de los datos
wilcox.test(Arbequina,Carolea, alternative="two.sided") # aplicamos el test de Mann Whitney
Wilcoxon bilateral
wilcox.test(Arbequina,Carolea, alternative="two.sided") # aplicamos el test de Mann Whitney
#Los datos no satisfacen el supuesto de normalidad distribucional, luego no puede aplicarse un test t.
library(RVAideMemoire)
install.packages("RVAdeMemoire")
install.packages("RVAideMemoire")
library(RVAideMemoire)
#te.aov<-aov(vitam  marca) # cargamos el análisis de la varianza en el objeto te.aov
#summary(te.aov) # pedimos la síntesis de la prueba
#bartlett.test(vitam,marca)
install.packages("Rcmdr)")
install.packages("Rcmdr")
install.packages("reshape2")
install.packages("car")
install.packages("car")
library("Rcmdr")
install.packages("nortest")
#Modelo con libreria  rpart con busqueda  Grid Search
#Estimo la ganancia con  Repeated Random Sub Sampling Validation   ( Monte Carlo Cross Validation )
#Por favor, no desesperarse por la ESPANTOSA granularidad de la Grid Search
#notar el uso de CPU y memoria RAM
#Si este programa se corta,  se lo debe volver a correr y automaticamente retoma desde donde llego la vez anterior
#Estadisticos y actuarios no entren en panico porque estamos entrenando y evaluando en el mismo mes, ya vamos a mejorar
#exp-1100
#source( "~/cloud/cloud1/codigoR/rpart/rpart_tune_gridsearch_01.r" )
#limpio la memoria
rm( list=ls() )
gc()
library( "rpart" )
library( "data.table" )
library( "caret" )
switch ( Sys.info()[['sysname']],
Windows = { directory.include  <-  "M:\\codigoR\\include\\"
directory.work     <-  "M:\\work\\"
directory.plan     <-  "M:\\plan\\"
directory.datasets <-  "M:\\datasets\\"
},
Darwin  = { directory.include  <-  "~/dm/codigoR/include/"
directory.work     <-  "~/dm/work/"
directory.plan     <-  "~/dm/plan/"
directory.datasets <-  "~/dm/datasets/"
},
Linux   = { directory.include  <-  "~/cloud/cloud1/codigoR/include/"
directory.work     <-  "~/cloud/cloud1/work/"
directory.plan     <-  "~/cloud/cloud1/plan/"
directory.datasets <-  "~/cloud/cloud1/datasets/"
}
)
setwd( directory.include )
source( "metrica.r" )
#Parametros entrada de nuestro dataset
karchivo_entrada      <-  "201802.txt"
kcampos_separador     <-  "\t"
kcampo_id             <-  "numero_de_cliente"
kclase_nomcampo       <-  "clase_ternaria"
kclase_valor_positivo <-  "BAJA+2"
kcampos_a_borrar      <-  c( kcampo_id )
#Parametros  Repeated Random Sub Sampling Validation
ktraining_prob        <-  0.70
ksemilla_azar         <-  c( 102191, 200177, 410551, 552581, 892237 )
karchivo_salida       <-  "hyperparameter_GLOBAL.txt"
#estos valores se graban en el archivo de salida
kexperimento          <-  1100
kclase                <-  "ternaria"
kprograma             <-  "rpart_tune_gridsearch_01.r"
kalgoritmo            <-  "rpart"
kbusqueda             <-  "gridsearch"
kestimacion           <-  "Montecarlo"
kobservaciones        <-  "5 semillas"
#------------------------------------------------------
#Genera el modelo usando una semilla
modelo_rpart_uno = function( psemilla, pmaxdepth, pminbucket, pminsplit, pcp )
{
set.seed( psemilla )
inTraining        <-  createDataPartition( dataset[ , get(kclase_nomcampo)],   p = ktraining_prob, list = FALSE)
dataset_training  <-  dataset[  inTraining, ]
dataset_testing   <-  dataset[ -inTraining, ]
# generacion del modelo
formula  <-  formula( paste(kclase_nomcampo, "~ .") )
t0       <-  Sys.time()
modelo   <-  rpart( formula,   data = dataset_training,  xval=0, maxdepth=pmaxdepth, minbucket=pminbucket, minsplit=pminsplit, cp=pcp )
t1       <-  Sys.time()
tiempo <-  as.numeric(  t1 - t0, units = "secs")
#aplico el modelo a datos nuevos
testing_prediccion  <- predict(  modelo, dataset_testing , type = "prob")
# calculo la ganancia normalizada  en testing
gan <-  fmetrica_ganancia_rpart( testing_prediccion[, kclase_valor_positivo ],  dataset_testing[ , get(kclase_nomcampo)] ) / ( 1- ktraining_prob )
# calculo el AUC en testing
auc <- fmetrica_auc_rpart( testing_prediccion[ ,kclase_valor_positivo],  dataset_testing[ , get(kclase_nomcampo)] )
return(  list( "ganancia"=gan,  "tiempo"= tiempo,  "auc"=auc )  )
}
#------------------------------------------------------
#corre  rpart  usando  las semillas, y promedia el resultado
modelo_rpart_ganancia = function( dataset, pmaxdepth, pminbucket, pminsplit, pcp  )
{
res  <-   lapply( ksemilla_azar, modelo_rpart_uno, pmaxdepth=pmaxdepth,  pminbucket=pminbucket, pminsplit=pminsplit, pcp=pcp )
return(  list( "ganancia" = unlist( lapply( res, '[[', "ganancia" ) ),
"tiempo"   = unlist( lapply( res, '[[', "tiempo" ) ),
"auc"      = unlist( lapply( res, '[[', "auc" ) )
)
)
}
#------------------------------------------------------
#cargo los datos
setwd( directory.datasets )
dataset <- fread( karchivo_entrada, header=TRUE, sep=kcampos_separador )
#borro las variables que no me interesan
dataset[ ,  (kcampos_a_borrar) := NULL    ]
#escribo los  titulos  del archivo salida
setwd( directory.work )
if( !file.exists( karchivo_salida) )
{
cat("experimento",
"metrica",
"metrica2",
"tiempo",
"parametros",
"fecha",
"clase", "programa", "algoritmo", "busqueda" , "estimacion",
"dataset_train", "dataset_test", "observaciones",
"\n", sep="\t", file=karchivo_salida, fill=FALSE, append=FALSE )
lineas_salida <- 0
} else
{
salida <-  read.table( karchivo_salida, header=TRUE, sep=kcampos_separador )
lineas_salida <- nrow( salida )
}
linea <- 1
for( vcp  in  c( 0, 0.0005,  0.001, 0.005 ) )
{
for( vminsplit  in  c(  2, 5, 10, 20, 50, 100, 200, 400, 500, 800, 1000 )  )
{
for( vminbucket  in  c( trunc(vminsplit/4), trunc(vminsplit/3) )  )
{
for(  vmaxdepth  in  c(  4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30 ) )
{
if( linea > lineas_salida )  #no volver a procesar si en la corrida anterior se llego a esa lina
{
res <- modelo_rpart_ganancia( dataset, pmaxdepth=vmaxdepth, pminbucket=vminbucket, pminsplit=vminsplit, pcp=vcp  )
#genero el string con los parametros
st_parametros = paste( "xval=",      0,          ", " ,
"maxdepth=",  vmaxdepth,  ", " ,
"minbucket=", vminbucket, ", " ,
"minsplit=",  vminsplit,  ", " ,
"cp=",        vcp,
sep = ""
)
cat( kexperimento,
mean(res$ganancia),
mean(res$auc),
sum(res$tiempo),
st_parametros,
format(Sys.time(), "%Y%m%d %H%M%S"),
kclase,
kprograma,
kalgoritmo,
kbusqueda,
kestimacion,
karchivo_entrada, karchivo_entrada,
kobservaciones,
"\n", sep="\t", file=karchivo_salida, fill=FALSE, append=TRUE
)
}
linea <- linea+1
}
}
}
}
#limpio la memoria
rm( list=ls() )
gc()
#salgo del R sin grabar el gigante entorno
quit( save="no" )
rm(list=ls())
setwd("E:/UBA/2018-II/DM en Ciencia y Tecnología/Ciclos Sueño")
## Load package
library(igraph)
# http://igraph.org/r/doc/
library(corrplot)
########################Generación Redes Promedio########################
N1 <- read.csv("N1promedio.csv",header=FALSE)
N2 <- read.csv("N2promedio.csv",header=FALSE)
N3 <- read.csv("N3promedio.csv",header=FALSE)
W <- read.csv("wpromedio.csv",header=FALSE)
W <- read.csv("Wpromedio.csv",header=FALSE)
N1 <- read.csv("N1promedio.csv",header=FALSE)
N2 <- read.csv("N2promedio.csv",header=FALSE)
N3 <- read.csv("N3promedio.csv",header=FALSE)
W <- W[-1, ]
W[1] <- NULL
N2 <- N2[-1, ]
N2[1] <- NULL
N3 <- N3[-1, ]
N3[1] <- NULL
aal <- read.csv("aal_extended.csv", header = F)
aalnames <- aal[,2]
N1 <- as.matrix(N1)
N2 <- as.matrix(N2)
N3 <- as.matrix(N3)
W <- as.matrix(W)
class(W) <- "numeric"
class(N2) <- "numeric"
class(N3) <- "numeric"
colnames(N1) <- aalnames
colnames(N2) <- aalnames
colnames(N3) <- aalnames
colnames(W) <- aalnames
#
rownames(N1) <- aalnames
rownames(N2) <- aalnames
rownames(N3) <- aalnames
rownames(W) <- aalnames
corrplot(W, is.corr=TRUE, title = "W")#, order="hclust")
N = dim(N1)[1]
Nmaxlinks = N*(N-1)
n = 1000
delta = n/Nmaxlinks
print(delta)
n = 1334
delta = n/Nmaxlinks
print(delta)
diag(W)<-0
tmp.W<-sort(as.vector(W),decreasing = TRUE)
ro.W = tmp.W[n]
Wb.W = (W>ro.W)
View(W)
View(W)
ro.W
Wb.W
View(Wb.W)
View(Wb.W)
ro.W
tmp.W[n]
pmax(W)
Wb.W
View(Wb.W)
View(Wb.W)
View(N1)
View(N1)
print(delta)
n = 700
delta = n/Nmaxlinks
print(delta)
delta = 0.038
diag(W)<-0
tmp.W<-sort(as.vector(W),decreasing = TRUE)
ro.W = tmp.W[delta]
Wb.W = (W>ro.W)
Wb.W
n = 700
delta = n/Nmaxlinks
print(delta)
delta = 0.038
diag(W)<-0
tmp.W<-sort(as.vector(W),decreasing = TRUE)
ro.W = tmp.W[n]
Wb.W = (W>ro.W)
Wb.W
diag(W)<-0
tmp.W<-sort(as.vector(W),decreasing = TRUE)
ro.W = tmp.W[delta]
Wb.W = (W>ro.W)
Wb.W
n = 700
delta = n/Nmaxlinks
print(delta)
n = Nmaxlinks*delta
n
delta = 0.038
n = Nmaxlinks*delta
n
rm(list=ls())
setwd("E:/UBA/2018-II/DM en Ciencia y Tecnología/Ciclos Sueño")
## Load package
library(igraph)
# http://igraph.org/r/doc/
library(corrplot)
W <- read.csv("Wpromedio.csv",header=FALSE)
N1 <- read.csv("N1promedio.csv",header=FALSE)
N2 <- read.csv("N2promedio.csv",header=FALSE)
N3 <- read.csv("N3promedio.csv",header=FALSE)
#Adjusting W, N2 and N3 as N1 for 116x116
W <- W[-1, ]
W[1] <- NULL
N2 <- N2[-1, ]
N2[1] <- NULL
N3 <- N3[-1, ]
N3[1] <- NULL
aal <- read.csv("aal_extended.csv", header = F)
aalnames <- aal[,2]
##
N1 <- as.matrix(N1)
N2 <- as.matrix(N2)
N3 <- as.matrix(N3)
W <- as.matrix(W)
class(W) <- "numeric"
class(N2) <- "numeric"
class(N3) <- "numeric"
colnames(N1) <- aalnames
colnames(N2) <- aalnames
colnames(N3) <- aalnames
colnames(W) <- aalnames
#
rownames(N1) <- aalnames
rownames(N2) <- aalnames
rownames(N3) <- aalnames
rownames(W) <- aalnames
#delta = 0.038
#n = Nmaxlinks*delta
delta = 0.038
n = Nmaxlinks*delta
N = dim(N1)[1]
Nmaxlinks = N*(N-1)
delta = 0.038
n = Nmaxlinks*delta
diag(W)<-0
tmp.W<-sort(as.vector(W),decreasing = TRUE)
ro.W = tmp.W[n]
Wb.W = (W>ro.W)
netW <- graph.adjacency(Wb.W,mode="undirected",diag = FALSE)
V(netW)$media <- aalnames
plot(netW)
vcount(netW)
ecount(netW)
n = 700
delta = n/Nmaxlinks
diag(W)<-0
tmp.W<-sort(as.vector(W),decreasing = TRUE)
ro.W = tmp.W[n]
Wb.W = (W>ro.W)
n = 7
delta = n/Nmaxlinks
#print(delta)
#delta = 0.038
#n = Nmaxlinks*delta
##############################Delta calculation##############################
diag(W)<-0
tmp.W<-sort(as.vector(W),decreasing = TRUE)
ro.W = tmp.W[n]
Wb.W = (W>ro.W)
netW <- graph.adjacency(Wb.W,mode="undirected",diag = FALSE)
V(netW)$media <- aalnames
plot(netW)
vcount(netW)
ecount(netW)
n = 7000
delta = n/Nmaxlinks
diag(W)<-0
tmp.W<-sort(as.vector(W),decreasing = TRUE)
ro.W = tmp.W[n]
Wb.W = (W>ro.W)
netW <- graph.adjacency(Wb.W,mode="undirected",diag = FALSE)
V(netW)$media <- aalnames
plot(netW)
vcount(netW)
ecount(netW)
is.simple(netW)
is.connected(netW)
diameter(netW)
graph.density(netN1)
######################## Taller Práctico######################################
par(mfrow = c(1,2))
hist(W[lower.tri(W)], main = "Histograma Relaciones W")
hist(N1[lower.tri(N1)], main = "Histograma Relaciones N1")
par(mfrow = c(1,1))
par(mfrow = c(1,2))
hist(N2[lower.tri(N2)], main = "Histograma Relaciones N2")
hist(N3[lower.tri(N3)], main = "Histograma Relaciones N3")
par(mfrow = c(1,1))
is.connected(netN1)
is.connected(netN2)
is.connected(netN3)
is.connected(netW)
transitivity(netW, type = "global")
par(mfrow = c(1,2))
hist(transitivity(netN1, type = "local"),
main = "N1", xlab = "coefs. de clustering")
hist(transitivity(netN1_umbral, type = "local"),
main = "N1 Umbral", xlab = "coefs. de clustering")
par(mfrow = c(1,1))
par(mfrow = c(1,2))
hist(transitivity(netW, type = "local"),
main = "W", xlab = "coefs. de clustering")
hist(transitivity(netN1, type = "local"),
main = "N1", xlab = "coefs. de clustering")
par(mfrow = c(1,1))
par(mfrow = c(1,2))
plot(degree.distribution(netW),
xlab = "Grados", ylab = "Proporción de nodos", type = "h", main ="W")
plot(degree.distribution(netN2),
xlab = "Grados", ylab = "Proporción de nodos", type = "h", main ="N1")
par(mfrow = c(1,1))
graph.density(netW)
mean(degree(netW))
delta = 0.038
n = Nmaxlinks*delta
delta = 0.038
n = Nmaxlinks*delta
n
netW <- graph.adjacency(Wb.W,mode="undirected",diag = FALSE)
V(netW)$media <- aalnames
plot(netW)
netW <- graph.adjacency(Wb.W,mode="undirected",diag = FALSE)
V(netW)$media <- aalnames
plot(netW)
rm(list=ls())
setwd("E:/UBA/2018-II/DM en Ciencia y Tecnología/Ciclos Sueño")
## Load package
library(igraph)
# http://igraph.org/r/doc/
library(corrplot)
W <- read.csv("Wpromedio.csv",header=FALSE)
N1 <- read.csv("N1promedio.csv",header=FALSE)
N2 <- read.csv("N2promedio.csv",header=FALSE)
N3 <- read.csv("N3promedio.csv",header=FALSE)
#Adjusting W, N2 and N3 as N1 for 116x116
W <- W[-1, ]
W[1] <- NULL
N2 <- N2[-1, ]
N2[1] <- NULL
N3 <- N3[-1, ]
N3[1] <- NULL
aal <- read.csv("aal_extended.csv", header = F)
aalnames <- aal[,2]
##
N1 <- as.matrix(N1)
N2 <- as.matrix(N2)
N3 <- as.matrix(N3)
W <- as.matrix(W)
class(W) <- "numeric"
class(N2) <- "numeric"
class(N3) <- "numeric"
colnames(N1) <- aalnames
colnames(N2) <- aalnames
colnames(N3) <- aalnames
colnames(W) <- aalnames
#
rownames(N1) <- aalnames
rownames(N2) <- aalnames
rownames(N3) <- aalnames
rownames(W) <- aalnames
View(N1)
View(N1)
View(N2)
View(N2)
View(N3)
View(N3)
View(W)
View(W)
N = dim(N1)[1]
Nmaxlinks = N*(N-1)
delta = 0.038
n = Nmaxlinks*delta
diag(W)<-0
tmp.W<-sort(as.vector(W),decreasing = TRUE)
ro.W = tmp.W[n]
Wb.W = (W>ro.W)
Wb.W
netW <- graph.adjacency(Wb.W,mode="undirected",diag = FALSE)
V(netW)$media <- aalnames
plot(netW)
library(igraph)
library(corrplot)
netW <- graph.adjacency(Wb.W,mode="undirected",diag = FALSE)
netW
V(netW)$media <- aalnames
plot(netW)
